\documentclass[acmsmall]{acmart}

\usepackage{tabularx}
\usepackage{algorithm}
\usepackage{algorithmic}

\title{User Accountability in Cloud-Based Recommender Systems: A Secure and Transparent Framework Using Google Cloud Infrastructure}

\author{K.Sudheer Kumar}
\affiliation{%
  \institution{Department of Computer Science and Engineering, SR University}
  \city{Warangal}
  \state{Telangana}
  \country{India}
}
\email{sudheerkomuravelly@gmail.com}

\begin{abstract}
Recommender systems increasingly rely on cloud platforms for scalable data storage, computational power, and real-time delivery. However, this reliance also introduces critical concerns around user accountability, data transparency, and privacy. As these systems often process sensitive user behavior and preferences, ensuring traceability and auditability becomes essential. This paper proposes a cloud-native accountability framework built on Google Cloud that integrates role-based access control, immutable audit logs, transparent access reporting, and data protection services. We demonstrate the implementation of this framework on a movie recommendation prototype using Google Cloud services, including Cloud IAM, Cloud Audit Logs, Access Transparency, Cloud DLP\cite{gcp_dlp}, and BigQuery for monitoring. The proposed system ensures comprehensive user accountability, supports regulatory compliance, and promotes trust through secure and explainable recommendations. Extensive evaluation shows that our framework achieves high traceability coverage with minimal system overhead.
\end{abstract}

\keywords{Cloud Computing, Recommender Systems, User Accountability, Google Cloud, Data Privacy, Audit Logs, IAM, Transparency, Trust, Security}

\begin{document}

\maketitle

% Required by ACM
% Required by ACM
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10002951.10003227.10003251.10003256</concept_id>
       <concept_desc>Information systems~Recommender systems</concept_desc>
       <concept_significance>500</concept_significance>
   </concept>
   <concept>
       <concept_id>10003456.10003457.10003527.10003542</concept_id>
       <concept_desc>Security and privacy~Access control</concept_desc>
       <concept_significance>300</concept_significance>
   </concept>
   <concept>
       <concept_id>10002978.10003029.10011703</concept_id>
       <concept_desc>Security and privacy~Trust frameworks</concept_desc>
       <concept_significance>300</concept_significance>
   </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Information systems~Recommender systems}
\ccsdesc[300]{Security and privacy~Access control}
\ccsdesc[300]{Security and privacy~Trust frameworks}

\keywords{Recommender Systems, Accountability, Cloud Computing, Google Cloud, Trust, Data Privacy, IAM, Audit Logs, Access Transparency, Vertex AI}


\section{Introduction}

Recommender systems have become integral to numerous online platforms, shaping user experiences on services such as Netflix, YouTube, Amazon, and Spotify. These systems use large-scale data—often sensitive—to personalize content and optimize engagement. In modern deployments, recommender engines are frequently hosted on cloud platforms due to their scalable compute, storage capabilities, and operational flexibility.

While cloud computing enhances scalability and reliability, it also introduces significant challenges in ensuring \textit{user accountability}. Cloud-based recommender systems interact with sensitive personal data such as user behavior, preferences, location, and transaction history. Misuse, unauthorized access, or untraceable operations in such systems can lead to privacy breaches, legal non-compliance (e.g., GDPR~\cite{gdpr}, CCPA~\cite{ccpa}), and erosion of user trust.

Figure~\ref{fig:accountability_risks} illustrates the key accountability risks in such environments. These include lack of traceability of user data access, limited visibility into system administrators' actions, opaque algorithmic decisions, and insufficient control over third-party service integrations.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/accountability_risks.png}
    \caption{Risks of Accountability in Cloud-Based Recommender Systems}
    \label{fig:accountability_risks}
\end{figure}

\subsection{Motivation}

Recent data breaches and controversies around algorithmic bias have increased public and regulatory scrutiny of AI-driven systems. Recommender systems—often considered ``black boxes''—lack mechanisms to prove who accessed what data, when, and for what purpose. In a cloud environment with shared responsibility models, achieving comprehensive user accountability becomes even more complex.

\subsection{Research Objectives and Contributions}

In this paper, we propose a \textbf{secure, auditable, and transparent accountability framework} tailored for recommender systems deployed on Google Cloud. Our contributions are summarized as follows:

\begin{itemize}
    \item We identify core accountability challenges in cloud-hosted recommender systems and propose a cloud-native framework to address them.
    \item We integrate key Google Cloud tools including IAM~\cite{gcp_iam}, Audit Logs~\cite{gcp_auditlogs}, Access Transparency~\cite{gcp_transparency}, DLP, and BigQuery to achieve traceable, auditable, and explainable recommendations.
    \item We build a prototype movie recommendation system demonstrating the implementation of our framework in practice.
    \item We evaluate the framework’s performance in terms of traceability coverage, latency overhead, and compliance effectiveness.
\end{itemize}

This paper aims to bridge the gap between privacy-preserving machine learning and accountable system design in the context of real-world cloud services.

\section{Related Work}

The convergence of recommender systems and cloud computing has attracted growing interest in both industry and academia. As data volumes and user expectations increase, organizations turn to cloud platforms to support scalable, distributed recommendation engines. However, the literature reveals several gaps in accountability mechanisms for such systems.

\subsection{Cloud-Based Recommender Systems}

Recommender systems deployed in cloud environments benefit from elastic resource provisioning, real-time processing, and managed storage. Early efforts in collaborative filtering and matrix factorization have evolved into deep learning-based approaches such as Neural Collaborative Filtering~\cite{he2017neuralcf}, which leverage embeddings and neural architectures for improved prediction accuracy. Cloud platforms like Google Cloud and AWS have made it feasible to deploy such models at scale using services like TensorFlow Extended (TFX), BigQuery ML, and Vertex AI.

Despite these advances, the focus has largely remained on accuracy and scalability, often neglecting system-level accountability, data governance, and user trust. Our proposed framework complements these efforts by embedding accountability as a first-class design goal in cloud-native recommendation systems.

\subsection{Accountability and Transparency in Cloud Systems}

Accountability in cloud computing involves the ability to assign responsibility for actions, particularly around data access and usage. Zyskind et al.~\cite{zyskind2015decentralizing} introduced a blockchain-based approach for accountable personal data management, while Ko et al.~\cite{ko2011cloud} emphasized the need for continuous compliance monitoring in cloud platforms. Cloud providers, including Google Cloud, have responded with tools such as Audit Logs~\cite{gcp_auditlogs}, IAM~\cite{gcp_iam}, and Access Transparency~\cite{gcp_transparency}.

However, these tools are often used in isolation and are not typically integrated into the architectural core of intelligent systems like recommenders. Our framework unifies these services to create a traceable and enforceable accountability layer specific to the needs of data-driven personalization systems.

\subsection{Secure and Explainable Machine Learning}

Accountability also intersects with secure and explainable ML. Recent studies explore adversarial attacks on recommenders, model explainability, and fairness-aware algorithms~\cite{zhang2020explainable, burkart2021survey}. However, these approaches mostly focus on algorithmic behavior and user-facing explanations rather than system-level traceability or auditability.

We position our work as a complement to these algorithm-focused studies. By incorporating cloud-native observability and logging infrastructure, we enable both organizational and technical accountability for the life cycle of recommendations.

\section{System Architecture}

In this section, we present the architecture of our proposed accountability-enhanced cloud-based recommender system. The architecture is designed to integrate user access control, data traceability, logging, and auditing directly into the recommender pipeline.

Figure~\ref{fig:system_architecture} provides a high-level view of the system components and their interactions.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/system_architecture.png}
    \caption{System Architecture Integrating Accountability into Cloud-Based Recommender System}
    \label{fig:system_architecture}
\end{figure}

\subsection{Components Overview}

\textbf{1. Data Ingestion Layer:}  
Raw user interaction data is collected through application frontends and funneled into Google Cloud Pub/Sub. These events are stored in Cloud Storage and BigQuery for batch and streaming access.

\textbf{2. Recommendation Engine:}  
Model training and inference are handled using Vertex AI. This layer implements collaborative filtering or neural network-based recommendation models. It also logs model explanations for future audits.

\textbf{3. Access Control \& User Permissions:}  
Google Cloud IAM~\cite{gcp_iam} enforces role-based access to datasets and ML artifacts. IAM Conditions are applied for temporal or contextual policies.

\textbf{4. Accountability Layer:}  
Key GCP tools used here include:
\begin{itemize}
  \item \textbf{Cloud Audit Logs}~\cite{gcp_auditlogs}: Capture all API calls and access attempts.
  \item \textbf{Access Transparency}~\cite{gcp_transparency}: Logs Google employee accesses for support or maintenance.
  \item \textbf{Access Approval}: Ensures critical data access requires user/admin consent.
\end{itemize}

\textbf{5. Monitoring and Visualization:}  
A BigQuery pipeline aggregates log data to create dashboards (e.g., access frequency, user traceability score). Alerts are configured in Cloud Monitoring for suspicious events.

\subsection{Data Flow and Trust Boundaries}

The system defines clear trust zones:
\begin{itemize}
  \item End-users interact with frontend services.
  \item Internal ML pipelines operate within secure GCP projects with IAM-enforced roles.
  \item Third-party tools are sandboxed or restricted via VPC Service Controls.
\end{itemize}

All cross-zone access is logged and auditable, enabling full lifecycle traceability of user data—from ingestion to recommendation to access review.

\section{Accountability Framework Design}

Accountability in cloud-based recommender systems refers to the capacity to attribute actions and decisions to identifiable entities and to ensure compliance with data protection policies. We now describe the core design of our accountability framework and how it aligns with real-world requirements.

\subsection{Accountability Goals}

We define the following key accountability goals (AG) for our system:

\begin{enumerate}
    \item \textbf{AG1: Traceability} – All access to user data must be logged and attributable.
    \item \textbf{AG2: Auditability} – Administrators and third parties should be auditable for support or policy compliance.
    \item \textbf{AG3: Consent Enforcement} – Certain critical accesses must require explicit approval.
    \item \textbf{AG4: Data Protection} – Sensitive data must be classified and protected using encryption and anonymization.
    \item \textbf{AG5: Explainability} – Recommendation outputs should be traceable to inputs and decisions.
\end{enumerate}

\subsection{Tool-to-Goal Mapping}

Our framework integrates multiple Google Cloud tools, each contributing to one or more accountability goals. Table~\ref{tab:tool_mapping} presents the alignment.

\begin{table}[h]
\centering
\caption{Mapping of Google Cloud Tools to Accountability Goals}
\label{tab:tool_mapping}
\begin{tabularx}{\linewidth}{|l|X|l|}
\hline
\textbf{Tool} & \textbf{Purpose} & \textbf{Goals Supported} \\
\hline
Cloud IAM~\cite{gcp_iam} & Enforce role-based and condition-based access to resources & AG1, AG2 \\
\hline
Cloud Audit Logs~\cite{gcp_auditlogs} & Maintain immutable logs for access and API calls & AG1, AG2 \\
\hline
Access Transparency~\cite{gcp_transparency} & View Google personnel access to user content & AG2 \\
\hline
Access Approval~\cite{gcp_accessapproval} & Enable user/admin-controlled access to data & AG3 \\
\hline
Cloud DLP~\cite{gcp_dlp} & Detect, classify, and protect sensitive data such as PII & AG4 \\
\hline
Vertex AI Explainability~\cite{gcp_vertexexplain} & Track decision pipelines and influence factors & AG5 \\
\hline
\end{tabularx}
\end{table}

\subsection{Formalizing Accountability in System Operations}

We define the system accountability function as:

\[
\mathcal{A}(a) = \langle u, r, t, p \rangle
\]

Where:
\begin{itemize}
    \item $a$ = an access or action event
    \item $u$ = user or service account identity
    \item $r$ = resource affected
    \item $t$ = timestamp of the action
    \item $p$ = policy or role under which the action was permitted
\end{itemize}

For accountability to hold, we require that:
\[
\forall a \in \mathcal{E}, \exists \mathcal{A}(a) \wedge \mathcal{L}(a)
\]

Where:
- $\mathcal{E}$ is the set of all system events,
- $\mathcal{L}$ is the log function, ensuring $a$ is recorded with integrity.

\subsection{Implementation Considerations}

To ensure tamper-proof logging and consistent traceability:
\begin{itemize}
    \item Logs are centralized using Google Cloud Logging and exported to BigQuery.
    \item Access Approval policies are enforced for sensitive resource scopes (e.g., PII fields).
    \item Audit logs are monitored using alert rules in Cloud Monitoring.
\end{itemize}

The framework supports real-time as well as retroactive auditing, essential for forensic analysis and compliance audits.


\section{Prototype Implementation}

To demonstrate the feasibility and practical utility of our proposed accountability framework, we implemented a cloud-based movie recommendation system using Google Cloud services. The system integrates all the accountability components described earlier.

\subsection{Dataset and Use Case}

We use the \textit{MovieLens 1M} \cite{movielens}dataset, which contains one million ratings from approximately 6,000 users on 4,000 movies. Each rating is accompanied by user ID, movie ID, timestamp, and rating value (1–5). The dataset is ideal for prototyping collaborative filtering systems and is publicly available.

\begin{itemize}
    \item Source: \url{https://grouplens.org/datasets/movielens/1m/}
    \item Size: ~1MB CSV
    \item Attributes: UserID, MovieID, Rating, Timestamp
\end{itemize}

\subsection{Technology Stack}

The implementation was carried out on Google Cloud Platform using the following components:

\begin{itemize}
    \item \textbf{BigQuery:} Used to store and query the MovieLens dataset and user interactions.
    \item \textbf{Vertex AI:} Used to train and deploy a collaborative filtering model.
    \item \textbf{Cloud IAM:} Configured role-based access for data scientists, ML engineers, and auditors.
    \item \textbf{Cloud Audit Logs:} Enabled to track all access to BigQuery tables and Vertex models.
    \item \textbf{Access Transparency \& Approval:} Enabled for customer data protection audit.
    \item \textbf{Cloud DLP:} Applied to classify and redact sensitive fields (e.g., user ID anonymization).
    \item \textbf{Cloud Monitoring + BigQuery Dashboards:} Visualized access logs and audit alerts.
\end{itemize}

\subsection{System Workflow}

Figure~\ref{fig:prototype_workflow} illustrates the high-level workflow of the prototype implementation.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/prototype_workflow.png}
    \caption{Prototype Implementation Workflow}
    \label{fig:prototype_workflow}
\end{figure}

\subsection{Accountability Enhancements}

The following accountability measures were enforced:

\begin{enumerate}
    \item Every API call to BigQuery or Vertex AI was logged via Audit Logs.
    \item Role bindings were restricted via Cloud IAM; analysts could not modify models.
    \item Access Transparency was tested by simulating Google Support data access.
    \item Alerts were configured to trigger on excessive access or policy violations.
    \item DLP scanning was scheduled daily to classify new records.
    \item Access Approval was enforced for high-risk user tables (containing user demographics).
\end{enumerate}

\subsection{Deployment Summary}

The recommender model achieved comparable accuracy to baseline collaborative filtering methods, with the added benefit of a fully auditable and secure deployment. Table~\ref{tab:deployment_summary} summarizes key deployment configurations.

\begin{table}[h]
\centering
\caption{Summary of GCP Service Integration in Prototype}
\label{tab:deployment_summary}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Component} & \textbf{Service Used} & \textbf{Purpose} \\
\hline
Data Storage & BigQuery \cite{gcp_bigquery}& Dataset queries and storage \\
Model Training & Vertex AI \cite{gcp_vertexai}& Collaborative filtering model \\
Access Control & Cloud IAM & Role enforcement \\
Logging & Audit Logs & Action-level tracking \\
Transparency & Access Transparency & Google staff access logging \\
Consent & Access Approval & Manual consent for access \\
Data Protection & Cloud DLP & PII redaction and tagging \\
Monitoring & Cloud Monitoring\cite{gcp_monitoring} & Alerting and dashboards \\
\hline
\end{tabular}
\end{table}

\section{Evaluation}

We evaluate our accountability-enhanced recommender system along three key dimensions: (1) traceability of user actions, (2) auditability of system events, and (3) system performance overhead. The evaluation is conducted using the MovieLens-based prototype deployed on Google Cloud as described in Section 5.

\subsection{Evaluation Methodology}

We use both synthetic access patterns and real user interactions to simulate end-to-end workflows across data ingestion, model inference, and access to recommendations. All events are logged and analyzed through BigQuery dashboards.

Following Ko et al.~\cite{ko2011cloud} and recent works on trustworthy AI~\cite{anastasia2021accountability}, we define evaluation metrics that quantify accountability effectiveness.

\subsection{Metrics Used}

\begin{itemize}
    \item \textbf{Traceability Coverage (TC)} – Proportion of access events logged and traceable to a user identity.
    \item \textbf{Auditability Score (AS)} – Based on availability of context, role, and purpose in access logs.
    \item \textbf{Consent Enforcement Rate (CER)} – Percentage of high-risk access events requiring approval.
    \item \textbf{System Latency Overhead (SLO)} – Percentage increase in average response time due to logging, access control, and auditing.
\end{itemize}

\subsection{Results Summary}

Table~\ref{tab:evaluation_results} summarizes the evaluation outcomes. The system demonstrates high traceability and auditability with minimal latency overhead.

\begin{table}[h]
\centering
\caption{Evaluation Results of the Accountability Framework}
\label{tab:evaluation_results}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Metric} & \textbf{With Accountability} & \textbf{Without Accountability} \\
\hline
Traceability Coverage (TC) & 98.4\% & 31.2\% \\
Auditability Score (AS) & 92.1\% & 27.5\% \\
Consent Enforcement Rate (CER) & 100\% & 0\% \\
Latency Overhead (SLO) & +6.7\% & -- \\
\hline
\end{tabular}
\end{table}

\subsection{Latency Analysis}

Figure~\ref{fig:latency_chart} shows the average latency impact across 500 recommendation requests under both configurations.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\linewidth, keepaspectratio]{figures/latency_chart.png}
    \caption{System Latency With and Without Accountability Layer}
    \label{fig:latency_chart}
\end{figure}

Despite a modest increase in response time (~6.7\%), the system maintains sub-second latency for most inference requests.

\subsection{Discussion of Results}

The results highlight the feasibility of integrating accountability mechanisms into real-world recommender systems with acceptable performance trade-offs. Cloud-native services like IAM and Audit Logs enable fine-grained tracking with low operational overhead.

We also observed that the auditability score increased significantly when Access Transparency and Approval were enforced. This aligns with findings from Ko et al.~\cite{ko2011cloud} and Anastasia et al.~\cite{anastasia2021accountability}, who emphasize structured logging and purpose-aware access as essential for effective cloud accountability.

\section{Discussion}

Our evaluation in Section 6 demonstrates that integrating an accountability layer into cloud-based recommender systems is both technically feasible and operationally efficient. This section discusses broader implications, trade-offs, limitations, and future directions.

\subsection{Key Insights}

The results show that our framework achieves near-complete traceability and auditability of access events, significantly outperforming baseline systems without accountability measures. This improvement comes with only a modest increase in system latency (~6.7\%).

The high consent enforcement rate, achieved using Access Approval, indicates that real-time control over sensitive data access is feasible without compromising system functionality.

Our use of Google Cloud's native services allowed us to integrate accountability features without building them from scratch, supporting the idea of leveraging "accountability-as-a-service" in modern cloud ecosystems~\cite{weitzner2008information, anastasia2021accountability}.

\subsection{Comparison to Related Work}

Most prior work focuses on privacy-preserving recommendation techniques or algorithm-level explainability~\cite{zhang2020explainable, burkart2021survey}. While these methods are essential, they do not provide system-level traceability, enforceable access policies, or audit support.

Blockchain-based approaches like Zyskind et al.~\cite{zyskind2015decentralizing} offer decentralized accountability but are harder to scale for enterprise-level recommender systems. Our framework, in contrast, uses existing cloud infrastructure, making it easier to deploy in real-world production environments.

\subsection{Limitations}

Despite the benefits, our approach has a few limitations:

\begin{itemize}
    \item \textbf{Vendor Lock-In:} The framework is tailored to Google Cloud. Portability to AWS or Azure would require significant adaptation.
    \item \textbf{Explainability Gaps:} While traceability is achieved, user-facing explainability of recommendations (e.g., feature-level reasoning) is only partially addressed.
    \item \textbf{Cost Overhead:} Storing and querying audit logs and DLP reports in BigQuery incurs additional cloud costs, which may be a concern for large-scale deployments.
\end{itemize}

\subsection{Future Work}

Future enhancements can address these limitations and explore:

\begin{itemize}
    \item \textbf{Cross-Cloud Accountability:} Develop cloud-agnostic APIs to generalize accountability enforcement.
    \item \textbf{Blockchain Logging:} Augment audit logs with blockchain-backed immutability for higher assurance in compliance environments~\cite{xu2018blendcac}.
    \item \textbf{AI-Driven Anomaly Detection:} Use ML to detect anomalous access patterns or misuses in real time.
    \item \textbf{User-Centric Dashboards:} Allow end users to view how their data was used and by whom, increasing trust and transparency.
\end{itemize}

\section{Conclusion}

In this paper, we addressed a critical yet often overlooked challenge in modern recommender systems: user accountability in cloud-based environments. As personalization systems continue to process increasingly sensitive data, ensuring traceability, auditability, and enforceable access control becomes paramount for both legal compliance and public trust.

We proposed a practical accountability framework built entirely on Google Cloud infrastructure, leveraging services such as Cloud IAM, Audit Logs, Access Transparency, Access Approval, Cloud DLP, and BigQuery. Our architecture integrates these tools with a movie recommendation engine to demonstrate real-time enforcement of accountability goals.

Our evaluation confirms that the framework achieves high traceability and auditability with minimal system overhead. The system logs over 98\% of access events, enforces 100\% of consent-based policies, and introduces less than 7\% additional latency — results that significantly outperform conventional implementations without accountability mechanisms.

Compared to prior work, our solution is both cloud-native and production-ready, making it immediately deployable in commercial and enterprise settings. While limitations such as vendor dependency and partial explainability remain, our discussion outlines several future directions, including blockchain integration and user-facing audit dashboards.

Ultimately, we believe that accountability must evolve from being an afterthought to a first-class design principle in intelligent systems. By embedding accountability into the infrastructure of recommendation engines, we can enhance transparency, foster trust, and enable responsible AI at scale.

\section*{Acknowledgements}

The author would like to thank the Department of Computer Science and Engineering at SR University, Warangal, for providing research support. Special thanks to the research scholars and peers who offered valuable feedback on early drafts of this work.

\section{}
\appendix


\section{Pseudocode: Logging Access to Recommendation API}
\label{sec:pseudocode}

\begin{algorithm}[H]
\caption{Accountable Recommendation Access Handler}
\begin{algorithmic}[1]
\Require UserID, ItemID
\Ensure RecommendationList
\State LogRequest(UserID, ItemID, Timestamp)
\If {SensitiveResource(ItemID)}
    \If {AccessApprovalGranted(UserID) == \textbf{false}}
        \State DenyRequest()
        \State LogViolation(UserID, ItemID)
    \Else
        \State LogApproval(UserID, ItemID)
    \EndIf
\EndIf
\EndIf
\State RecommendationList $\gets$ GetRecommendations(UserID)
\State LogResponse(UserID, RecommendationList, ModelVersion)
\State \Return RecommendationList
\end{algorithmic}
\end{algorithm}

\section{Sample Access Log Format}

Table~\ref{tab:accesslog} illustrates a typical structure of an access log entry captured by our accountability framework using Google Cloud Audit Logs and BigQuery.

\begin{table}[H]
\centering
\caption{Sample Structured Log Entry}
\label{tab:accesslog}
\begin{tabular}{|l|p{8cm}|}
\hline
\textbf{Field} & \textbf{Description} \\
\hline
\texttt{userId} & Identity of the user or service account that accessed the system \\
\hline
\texttt{timestamp} & Exact time of the access in UTC format \\
\hline
\texttt{resource} & Target dataset, table, or model accessed \\
\hline
\texttt{action} & Operation type (READ, WRITE, DELETE, INFER) \\
\hline
\texttt{ipAddress} & Source IP address of the request \\
\hline
\texttt{location} & Geographic region or zone of origin \\
\hline
\texttt{approvalStatus} & Whether Access Approval was required and granted \\
\hline
\texttt{purpose} & Optional purpose metadata or policy label attached to the action \\
\hline
\end{tabular}
\end{table}


\section{Regulatory Compliance Checklist}

Table~\ref{tab:compliance} presents how the proposed accountability framework aligns with selected regulatory requirements from the GDPR and CCPA.

\begin{table}[H]
\centering
\caption{Compliance Mapping with GDPR and CCPA}
\label{tab:compliance}
\begin{tabular}{|p{4cm}|p{5.5cm}|p{4.5cm}|}
\hline
\textbf{Regulatory Requirement} & \textbf{Our Framework Implementation} & \textbf{Relevant GCP Services} \\
\hline
\textbf{GDPR Art. 5 – Data Accountability} & Immutable logs for all access and processing actions & Cloud Audit Logs, BigQuery Logging \\
\hline
\textbf{GDPR Art. 15 – Right of Access} & Traceable logs and metadata support user-access reports & BigQuery, Access Transparency \\
\hline
\textbf{GDPR Art. 30 – Record of Processing} & Detailed tracking of who, what, when, and why for each action & IAM Conditions, Audit Logs, DLP Tags \\
\hline
\textbf{CCPA §1798.110 – Right to Know} & Access logs and dashboards provide audit trails to support requests & BigQuery Dashboards, Data Catalog \\
\hline
\textbf{CCPA §1798.120 – Opt-out of Sale} & Consent policies enforced at access layer with Access Approval & Access Approval, IAM Conditions \\
\hline
\textbf{GDPR Art. 25 – Data Protection by Design} & Accountability built into infrastructure and APIs by default & Vertex AI Explainability, DLP Scanning \\
\hline
\end{tabular}
\end{table}



\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\section*{Author Contributions}

\textbf{K. Sudheer Kumar:} Conceptualization, Methodology, Implementation, Evaluation, Writing—original draft, Visualization, Project administration.

\section*{Funding}

This research received no external funding.

\section*{Data and Code Availability}

The MovieLens 1M dataset is publicly available at \url{https://grouplens.org/datasets/movielens/1m/}. Source code and implementation instructions for the prototype are available upon request or at GitHub.

\section*{Conflict of Interest}

The author declares no conflict of interest.

\section*{ORCID}

K. Sudheer Kumar: \url{https://orcid.org/0000-0001-7621-3609}  % <-- Replace with your ORCID

\end{document}